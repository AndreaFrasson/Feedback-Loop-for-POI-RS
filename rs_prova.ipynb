{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "\n",
    "from logging import getLogger\n",
    "from recbole.model.general_recommender import BPR\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "\n",
    "import torch\n",
    "from recbole.data.interaction import Interaction\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import make_atomic_files as at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "at.og_atomic_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_history(inter):\n",
    "    visits = []\n",
    "    set_uid = set(inter['uid:token'])\n",
    "    for u in set_uid:\n",
    "        visits.append(inter[inter['uid:token'] == u]['venue_id:token'].values.tolist())\n",
    "    \n",
    "    return np.array(visits)\n",
    "\n",
    "inter = pd.read_csv('foursquare/foursquare.inter', sep='\\t')\n",
    "\n",
    "visits = get_history(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recbole.trainer import HyperTuning\n",
    "from recbole.utils import init_seed\n",
    "import os\n",
    "from recbole.utils import get_model, get_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(config_dict=None, config_file_list=None):\n",
    "    k = 10\n",
    "\n",
    "    config = Config(config_dict=config_dict, config_file_list=['foursquare_general.yaml'])\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    dataset = create_dataset(config)\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "    model_name = config['model']\n",
    "    model = get_model(model_name)(config, train_data._dataset).to(config['device'])\n",
    "    trainer = get_trainer(config['MODEL_TYPE'], config['model'])(config, model)\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "    test_result = trainer.evaluate(test_data)\n",
    "\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'best_valid_score': best_valid_score,\n",
    "        'valid_score_bigger': config['valid_metric_bigger'],\n",
    "        'best_valid_result': best_valid_result,\n",
    "        'test_result': test_result\n",
    "    }\n",
    "\n",
    "hp = HyperTuning(objective_function=objective_function, algo='random', early_stop=10,\n",
    "                max_evals=100, params_file='bpr.hyper', fixed_config_file_list=['foursquare_general.yaml'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running parameters:                                    \n",
      "{'dynamic': False, 'embedding_size': 128, 'learning_rate': 0.06546900811244058, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 1, 'train_batch_size': 2048}\n",
      "current best valid score: 0.0037                       \n",
      "current best valid result:                             \n",
      "OrderedDict([('hit@10', 0.0037), ('precision@10', 0.0004)])\n",
      "current test result:                                   \n",
      "OrderedDict([('hit@10', 0.0028), ('precision@10', 0.0003)])\n",
      "running parameters:                                                   \n",
      "{'dynamic': False, 'embedding_size': 128, 'learning_rate': 0.42166822630766093, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 0, 'train_batch_size': 2048}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 64, 'learning_rate': 0.010544030274656958, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 5, 'train_batch_size': 2048}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 64, 'learning_rate': 0.010660649209692947, 'mlp_hidden_size': '[128,128]', 'sample_num': 5, 'train_batch_size': 2048}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.039634802628553005, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 5, 'train_batch_size': 2048}\n",
      "running parameters:                                                   \n",
      "{'dynamic': False, 'embedding_size': 128, 'learning_rate': 0.07885866707608949, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 5, 'train_batch_size': 4096}\n",
      "current best valid score: 0.0046                                      \n",
      "current best valid result:                                            \n",
      "OrderedDict([('hit@10', 0.0046), ('precision@10', 0.0005)])           \n",
      "current test result:                                                  \n",
      "OrderedDict([('hit@10', 0.0009), ('precision@10', 0.0001)])           \n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 96, 'learning_rate': 0.725746824599686, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 0, 'train_batch_size': 2048}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 64, 'learning_rate': 0.012539320773064975, 'mlp_hidden_size': '[128,128]', 'sample_num': 0, 'train_batch_size': 4096}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 64, 'learning_rate': 0.029068725350009032, 'mlp_hidden_size': '[128,128]', 'sample_num': 1, 'train_batch_size': 4096}\n",
      "running parameters:                                                   \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.008779139724612361, 'mlp_hidden_size': '[128,128]', 'sample_num': 5, 'train_batch_size': 2048}\n",
      "running parameters:                                                    \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.029147704489904988, 'mlp_hidden_size': '[128,128]', 'sample_num': 0, 'train_batch_size': 2048}\n",
      "running parameters:                                                    \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.005809359390302234, 'mlp_hidden_size': '[128,128]', 'sample_num': 0, 'train_batch_size': 4096}\n",
      "running parameters:                                                    \n",
      "{'dynamic': False, 'embedding_size': 128, 'learning_rate': 0.005904662199625913, 'mlp_hidden_size': '[128,128]', 'sample_num': 0, 'train_batch_size': 4096}\n",
      "running parameters:                                                    \n",
      "{'dynamic': False, 'embedding_size': 96, 'learning_rate': 0.053886238100097364, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 1, 'train_batch_size': 2048}\n",
      "running parameters:                                                    \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.011853316240562121, 'mlp_hidden_size': '[128,128]', 'sample_num': 5, 'train_batch_size': 4096}\n",
      "running parameters:                                                    \n",
      "{'dynamic': True, 'embedding_size': 128, 'learning_rate': 0.0006366139186746057, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 1, 'train_batch_size': 4096}\n",
      " 16%|█▌        | 16/100 [03:59<20:56, 14.96s/trial, best loss: -0.0046]\n",
      "{'dynamic': False, 'embedding_size': 128, 'learning_rate': 0.07885866707608949, 'mlp_hidden_size': '[64,64,64]', 'sample_num': 5, 'train_batch_size': 4096}\n"
     ]
    }
   ],
   "source": [
    "hp.run()\n",
    "params = hp.best_params\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train General Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('hit@10', 0.0009), ('precision@10', 0.0001)])\n"
     ]
    }
   ],
   "source": [
    "# configurations initialization\n",
    "k = 10\n",
    "\n",
    "config = Config(model='BPR', dataset='foursquare', config_file_list=['foursquare_general.yaml'], config_dict = params)\n",
    "\n",
    "# init random seed\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "\n",
    "# dataset creating and filtering\n",
    "dataset = create_dataset(config)\n",
    "\n",
    "# dataset splitting\n",
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "model = BPR(config, train_data.dataset).to(config['device'])\n",
    "\n",
    "# trainer loading and initialization\n",
    "trainer = Trainer(config, model)\n",
    "\n",
    "# model training\n",
    "best_valid_score, best_valid_result = trainer.fit(train_data, valid_data)\n",
    "\n",
    "# model evaluation\n",
    "test_result = trainer.evaluate(test_data)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27898 1083\n",
      "torch.Size([1083, 27899])\n",
      "102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2094/147712770.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    }
   ],
   "source": [
    "visits = get_history(inter)\n",
    "\n",
    "unique_users = list(set(inter['uid:token']))\n",
    "unique_locations = list(set(inter['venue_id:token']))\n",
    "print(len(unique_locations), len(unique_users))\n",
    "\n",
    "current_time = max(inter['timestamp:token'])+1\n",
    "\n",
    "#make prediction for users\n",
    "input_inter = Interaction({\n",
    "    'uid': torch.tensor(unique_users),\n",
    "    'venue_id': torch.tensor(visits)\n",
    "})\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = model.full_sort_predict(input_inter).reshape((len(unique_users), -1))\n",
    "\n",
    "#length |items| + 1 because of the padding\n",
    "print(scores.shape)\n",
    "\n",
    "# get the 10 items with highest scores\n",
    "rec_list = np.argsort(scores, axis = 1)[:, -k:]\n",
    "\n",
    "# select one item at random for each user\n",
    "def random_choice(a):\n",
    "    # select one item, but then translated back\n",
    "    r_c = np.random.choice(a, 1)\n",
    "    return int(r_c) - 1\n",
    "\n",
    "random_item = np.apply_along_axis(random_choice, 1, rec_list)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27898 1083\n"
     ]
    }
   ],
   "source": [
    "new_locations = pd.DataFrame({'uid:token': unique_users, 'venue_id:token':random_item.tolist(), 'timestamp:token':[current_time]*len(random_item)}, columns=['uid:token', 'venue_id:token', 'timestamp:token'])\n",
    "new_locations.head()\n",
    "\n",
    "inter = pd.concat([inter, new_locations], axis = 0).reset_index(drop = True)\n",
    "inter.sort_values(by=['uid:token', 'timestamp:token'], inplace=True)\n",
    "\n",
    "unique_users = list(set(inter['uid:token']))\n",
    "unique_locations = list(set(inter['venue_id:token']))\n",
    "print(len(unique_locations), len(unique_users))\n",
    "\n",
    "inter.to_csv('foursquare/foursquare.inter', index=False, sep = '\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
