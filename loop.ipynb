{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "\n",
    "from logging import getLogger\n",
    "from recbole.model.general_recommender import BPR\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "\n",
    "import torch\n",
    "from recbole.data.interaction import Interaction\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# build the first atomic files from the foursquare dataset\n",
    "def preprocess():\n",
    "    foursquare = pd.read_csv('data/foursquare_complete.csv')\n",
    "    foursquare['geometry'] = list(zip(foursquare['lat'], foursquare['lon']))\n",
    "\n",
    "    # encoding category\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(foursquare['venue_category_name'])\n",
    "    foursquare['venue_category_name'] = enc.transform(foursquare['venue_category_name'])\n",
    "\n",
    "    mapping = dict(zip(range(len(enc.classes_)), enc.classes_))\n",
    "    with open('name_category.pkl', 'wb') as f:\n",
    "        pickle.dump(mapping, f)\n",
    "\n",
    "\n",
    "    #equal length in all of the trajectory\n",
    "    min_len = float('inf')\n",
    "    set_uid = set(foursquare['uid'])\n",
    "    for u in set_uid:\n",
    "        min_len = min(min_len, len(foursquare[foursquare['uid'] == u]))\n",
    "\n",
    "    new_df = []\n",
    "    for u in set_uid:\n",
    "        to_append = foursquare[foursquare['uid'] == u].iloc[:min_len, :].values.tolist()\n",
    "        for r in to_append:\n",
    "            new_df.append(r)\n",
    "\n",
    "    new_df = pd.DataFrame(new_df, columns=foursquare.columns)\n",
    "\n",
    "    #timestamp\n",
    "    new_df['timestamp'] = np.arange(0, 100).tolist() * len(set_uid)\n",
    "\n",
    "    #inter file for recbole\n",
    "    red_df = new_df[['uid', 'venue_id', 'timestamp', 'venue_category_name']].copy()\n",
    "    red_df.columns = ('uid:token', 'venue_id:token', 'timestamp:token', 'venue_category_name:token')\n",
    "\n",
    "    # encoding ids\n",
    "    enc = LabelEncoder()\n",
    "    enc.fit(red_df['venue_id:token'])\n",
    "    red_df['venue_id:token'] = enc.transform(red_df['venue_id:token'])\n",
    "\n",
    "    mapping = dict(zip(range(len(enc.classes_)), enc.classes_))\n",
    "    with open('id_category.pkl', 'wb') as f:\n",
    "        pickle.dump(mapping, f)\n",
    "\n",
    "    # interaction file\n",
    "    red_df[['uid:token', 'venue_id:token', 'timestamp:token']].to_csv('foursquare/foursquare.inter', index=False, sep = '\\t')\n",
    "\n",
    "    #item file for recbole\n",
    "    items = red_df[['venue_id:token', 'venue_category_name:token']].drop_duplicates()\n",
    "    items.to_csv('foursquare/foursquare.item', index = False, sep='\\t')\n",
    "\n",
    "    #user file\n",
    "    pd.DataFrame(set(red_df['uid:token']), columns=['uid:token']).to_csv('foursquare/foursquare.user', index=False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the RS model\n",
    "def train_model(config_dict, k = 10):\n",
    "    # configurations initialization\n",
    "\n",
    "    config = Config(model='BPR', dataset='foursquare', config_file_list=['foursquare_general.yaml'], config_dict = config_dict)\n",
    "\n",
    "    # init random seed\n",
    "    init_seed(config['seed'], config['reproducibility'])\n",
    "    # logger initialization\n",
    "    init_logger(config)\n",
    "    logger = getLogger()\n",
    "\n",
    "    # write config info into log\n",
    "    logger.info(config)\n",
    "\n",
    "    # dataset creating and filtering\n",
    "    dataset = create_dataset(config)\n",
    "    logger.info(dataset)\n",
    "\n",
    "    # dataset splitting\n",
    "    train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "\n",
    "    # model loading and initialization\n",
    "    model = BPR(config, train_data.dataset).to(config['device'])\n",
    "    logger.info(model)\n",
    "\n",
    "    # trainer loading and initialization\n",
    "    trainer = Trainer(config, model)\n",
    "\n",
    "    # model training\n",
    "    best_valid_score, best_valid_result = trainer.fit(train_data, valid_data, verbose=False)\n",
    "\n",
    "    # model evaluation\n",
    "    test_result = trainer.evaluate(test_data)\n",
    "    print(test_result)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# for each user, get the last n (history_length) visits\n",
    "def get_history(interaction, history_length):\n",
    "    visits = []\n",
    "    set_uid = set(interaction['uid:token'])\n",
    "    for u in set_uid:\n",
    "        visits.append(interaction[interaction['uid:token'] == u]['venue_id:token'].values.tolist()[-history_length:])\n",
    "    \n",
    "    return visits\n",
    "\n",
    "\n",
    "# predict the nest visit for all the users, select the k locations with highest score\n",
    "def predict_k(model, users, visits, k): \n",
    "    #make prediction for users\n",
    "\n",
    "    input_inter = Interaction({\n",
    "        'uid': torch.tensor(users),\n",
    "        'venue_id': torch.tensor(visits),\n",
    "    })\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model.full_sort_predict(input_inter).reshape((len(users), -1))\n",
    "\n",
    "    # get the 10 items with highest scores\n",
    "    return np.argsort(scores, axis = 1)[:, -k:]\n",
    "\n",
    "\n",
    "\n",
    "# select one item in an array a, get its correct id\n",
    "def random_choice(a):\n",
    "    # select one item, but then translate it back\n",
    "    r_c = np.random.choice(a, 1)\n",
    "    return int(r_c) - 1\n",
    "\n",
    "\n",
    "# save the new interaction file\n",
    "def new_atomic_files(interaction):\n",
    "    try:\n",
    "        interaction.to_csv('foursquare/foursquare.inter', index=False, sep = '\\t')\n",
    "        print('saved')\n",
    "    except:\n",
    "        print('Error saving the interaction file.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------- iteration number:  0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18 May 11:00    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 1234\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/andreafrasson/Desktop/tesi/Feedback-Loop-for-POI-RS/foursquare\n",
      "checkpoint_dir = saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 4096\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [10]\n",
      "valid_metric = MRR@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = uid\n",
      "ITEM_ID_FIELD = venue_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['uid', 'venue_id', 'timestamp']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "18 May 11:00    INFO  foursquare\n",
      "The number of users: 1084\n",
      "Average actions of users: 100.0\n",
      "The number of items: 27899\n",
      "Average actions of items: 3.8819987095849164\n",
      "The number of inters: 108300\n",
      "The sparsity of the dataset: 99.6418948740905%\n",
      "Remain Fields: ['uid', 'venue_id', 'timestamp']\n",
      "18 May 11:00    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "18 May 11:00    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "18 May 11:00    INFO  BPR(\n",
      "  (user_embedding): Embedding(1084, 64)\n",
      "  (item_embedding): Embedding(27899, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 1854912\n",
      "18 May 11:00    INFO  Loading model structure and parameters from saved/BPR-May-18-2024_11-00-00.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('recall@10', 0.398), ('mrr@10', 0.3578), ('ndcg@10', 0.3673), ('hit@10', 0.398), ('precision@10', 0.0398)])\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2119/3090270459.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "\n",
      "--------- iteration number:  1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2119/3090270459.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "\n",
      "--------- iteration number:  2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2119/3090270459.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "\n",
      "--------- iteration number:  3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18 May 11:00    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 1234\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/andreafrasson/Desktop/tesi/Feedback-Loop-for-POI-RS/foursquare\n",
      "checkpoint_dir = saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 4096\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}\n",
      "repeatable = False\n",
      "metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision']\n",
      "topk = [10]\n",
      "valid_metric = MRR@10\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 4096\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = uid\n",
      "ITEM_ID_FIELD = venue_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = {'inter': ['uid', 'venue_id', 'timestamp']}\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 64\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.GENERAL\n",
      "MODEL_INPUT_TYPE = InputType.PAIRWISE\n",
      "eval_type = EvaluatorType.RANKING\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "device = cpu\n",
      "valid_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'uniform', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "18 May 11:00    INFO  foursquare\n",
      "The number of users: 1084\n",
      "Average actions of users: 103.0\n",
      "The number of items: 27899\n",
      "Average actions of items: 3.998458670872464\n",
      "The number of inters: 111549\n",
      "The sparsity of the dataset: 99.63115172031321%\n",
      "Remain Fields: ['uid', 'venue_id', 'timestamp']\n",
      "18 May 11:00    INFO  [Training]: train_batch_size = [4096] train_neg_sample_args: [{'distribution': 'uniform', 'sample_num': 1, 'alpha': 1.0, 'dynamic': False, 'candidate_num': 0}]\n",
      "18 May 11:00    INFO  [Evaluation]: eval_batch_size = [4096] eval_args: [{'split': {'LS': 'valid_and_test'}, 'order': 'TO', 'group_by': 'user', 'mode': {'valid': 'full', 'test': 'full'}}]\n",
      "18 May 11:00    INFO  BPR(\n",
      "  (user_embedding): Embedding(1084, 64)\n",
      "  (item_embedding): Embedding(27899, 64)\n",
      "  (loss): BPRLoss()\n",
      ")\n",
      "Trainable parameters: 1854912\n",
      "18 May 11:00    INFO  Loading model structure and parameters from saved/BPR-May-18-2024_11-00-17.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('recall@10', 0.0), ('mrr@10', 0.0), ('ndcg@10', 0.0), ('hit@10', 0.0), ('precision@10', 0.0)])\n",
      "103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2119/3090270459.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "\n",
      "--------- iteration number:  4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/mkx2sfs13pqgtr19k0_pc89h0000gn/T/ipykernel_2119/3090270459.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(r_c) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "\n",
      "--------- iteration number:  5\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: argsort() missing 1 required positional arguments: \"stable\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m visits \u001b[38;5;241m=\u001b[39m get_history(interaction, training_history_length)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m#recommendation list\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m rec_list \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#select one item in the list\u001b[39;00m\n\u001b[1;32m     39\u001b[0m random_item \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(random_choice, \u001b[38;5;241m1\u001b[39m, rec_list)\n",
      "Cell \u001b[0;32mIn[7], line 62\u001b[0m, in \u001b[0;36mpredict_k\u001b[0;34m(model, users, visits, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m     scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfull_sort_predict(input_inter)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(users), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# get the 10 items with highest scores\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39mk:]\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1133\u001b[0m, in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margsort\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margsort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:68\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess()\n",
    "\n",
    "k = 10\n",
    "config_dict = {\n",
    "    #environment settings\n",
    "    'seed': 1234,\n",
    "    'reproducibility': True,\n",
    "    'data_path': os.getcwd(), \n",
    "    'topk': k\n",
    "}\n",
    "\n",
    "m = 3\n",
    "MaxIt = 20\n",
    "interaction = pd.read_csv('foursquare/foursquare.inter', sep='\\t')\n",
    "\n",
    "users = list(set(interaction['uid:token']))\n",
    "locations = list(set(interaction['venue_id:token']))\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < MaxIt:\n",
    "    print('')\n",
    "    print('--------- iteration number: ', i)\n",
    "    print('')\n",
    "\n",
    "    if i % m == 0:  \n",
    "        # train the model\n",
    "        model = train_model(config_dict)\n",
    "        training_history_length = max(interaction['timestamp:token'])+1\n",
    "        print(training_history_length)\n",
    "\n",
    "    #get the users' history\n",
    "    visits = get_history(interaction, training_history_length)\n",
    "\n",
    "    #recommendation list\n",
    "    rec_list = predict_k(model, users, visits, k)\n",
    "\n",
    "    #select one item in the list\n",
    "    random_item = np.apply_along_axis(random_choice, 1, rec_list)\n",
    "\n",
    "    current_time = max(interaction['timestamp:token'])+1\n",
    "    ### update the training set\n",
    "    new_locations = pd.DataFrame({'uid:token': users, 'venue_id:token':random_item.tolist(), \n",
    "                                    'timestamp:token':[current_time]*len(random_item)}, \n",
    "                                    columns=['uid:token', 'venue_id:token', 'timestamp:token'])\n",
    "    \n",
    "    interaction = pd.concat([interaction, new_locations], axis = 0).reset_index(drop = True)\n",
    "    interaction.sort_values(by=['uid:token', 'timestamp:token'], inplace=True)\n",
    "\n",
    "    new_atomic_files(interaction)\n",
    "\n",
    "    card_loc = len(set(new_locations['venue_id:token']))\n",
    "    \n",
    "\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
